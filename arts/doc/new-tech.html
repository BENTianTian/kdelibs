<HTML>
<TITLE>
Work in progress
</TITLE>
<BODY bgcolor="#EAEEEE">
<p>
<h3>Work in progress</h3>
<p>
This is a draft document which tries to give you an overview how new
technologies will be integrated in aRts. Namely, it does cover the following:
<ul>
<li>how interfaces work
<li>codecs - decoding of mp3 or wav streams in a form that they can be used
    as data
<li>video
<li>threading
<li>synchronization
<li>dynamic expansion/masquerading
<li>dynamic composition
<li>gui
<li>midi
</ul>
<p>
This is work in progress. However, it should be the base if you want to see
new technology in aRts. It should give you a general idea how these problems
will be adressed.
<p>
However, feel free to correct anything you see here.
<p>
Things that will be use aRts technology (so please: coordinate your effords):
<p>
<ul>
<li>KPhone   (voice over IP)
<li>Noatun   (video / audio player)
<li>aRtscontrol (sound server control program, for scopes)
<li>Brahms   (music sequencer)
<li>kaiman   (KDE2 media player - kmedia2 compliant)
<li>mpglib/kmpg   (mpg audio and video playing technology)
<li>SDL      (direct media layer for games not yet started but maybe nice)
<li>electric ears (author contacted me - status unknown)
</ul>
<p>
<h4>how interfaces work</h4>
<p>
MCOP interfaces are the base of the aRts concept. They are the network
transparent equivalent to C++ classes. Whenever possible you should orient
your design towards interfaces. Interfaces consist of four parts:
<p>
<ul>
<li>synchronous streams
<li>asynchronous streams
<li>methods
<li>attributes
</ul>
<p>
These can be mixed in any way you like. New technologies should be defined
in terms of interfaces. Read the sections about asynchronous streams and
synchronous streams, as well as the KMedia2 interfaces, which are a good
example how such things work
<p>
Interfaces are specified in .idl code and run through the mcopidl compiler.
You derive the Interfacename_impl class to implement them, and use
REGISTER_IMPLEMENTATION(Interfacename_impl) to insert your object
implementations into the MCOP object system.
<p>
<h4>codecs - data-decoding</h4>
<p>
The kmedia2 interfaces allow you to ignore that wav files, mp3s and whatever
consist of data streams. Instead, you only implement methods to play them.
<p>
Thus, you can write a wave loading routine in a way that you can play wave
files (as PlayObject), but nobody else can use your code.
<p>
Asynchronous streams would be the alternative. You define an interface which
allows you to pass data blocks in, and get data blocks out. This looks like
that in MCOP:
<p>
<pre>
interface Codec {
  in async byte stream indata;
  out async byte stream outdata;
};
</pre>
<p>
Of course codecs could also provide attributes to emit additional data, such
as format information.
<p>
<pre>
interface ByteAudioCodec {
  in async byte stream indata;
  out async byte stream outdata;
  readonly attribute samplingRate, bits, channels;
};
</pre>
<p>
This ByteAudioCodec for instance could be connected to a ByteStreamToAudio
object, to make real float audio.
<p>
Of course, other Codec types could involve directly emitting video data, 
such as
<p>
<pre>
interface VideoCodec {
  in async byte stream indata;
  out video stream outdata;      /* note: video streams do not exist yet */
};
</pre>
<p>
Most likely, a codec concept should be employed rather than the "you know
how to play and I don't" way for instance WavPlayObject currently uses.
However, somebody needs to sit down and do some experiments before an API
can be finalized.
<p>
<p>
<h4>video</h4>
<p>
My idea is to provide video as asynchronous streams of some native MCOP data
type which contains images. This data type is to be created yet. Doing so,
plugins which deal with video images could be connected the same way audio
plugins can be connected.
<p>
There are a few things that are important not to leave out, namely:
<p>
<ul>
<li>there are RGB and YUV colorspaces
<li>the format should be somehow tagged to the stream
<li>synchronization is important
</ul>
<p>
My idea is to leave it possible to reimplement the VideoFrame class so that
it can store stuff in a shared memory segment. Doing so, even video streaming
between different processes would be possible without too much pain.
<p>
However, the standard situation for video is that things are in the same
process, from the decoding to the rendering.
<p>
I have done a prototypic video streaming implementation, which you can download
<a href="http://space.twc.de/~stefan/kde/download/video-quickdraw.tar.gz">here
</a>. This would need to be integrated into MCOP after some experiments.
<p>
A rendering component should be provided that supports XMITSHM (with RGB
and YUV), Martin Vogt told me he is working on such a thing.
<p>
<h4>threading</h4>
<p>
Currently, MCOP is all single threaded. Maybe for video we will no longer be
able to get around threading. Ok. There are a few things that should be
treated carefully:
<ul>
<li>SmartWrappers - they are not threadsafe due to non-safe reference counting
    and similar
<li>Dispatcher / I/O - also not threadsafe
</ul>
However, what I could imagine is to make selected modules threadsafe, for
both, synchronous and asynchronous streaming. That way - with a thread aware
flow system, you could schedule the signal flow over two or more processors.
<p>
This would also help audio a lot on multiprocessor things.
<p>
How it would work:
<p>
<ul>
<li>The Flow System decides which modules should calculate what - that is
    <ul>
	<li>video frames (with process_indata method)
	<li>synchronous audio streams (calculateBlock)
	<li>other asynchronous streams, mainly byte streams
	</ul>
<li>Modules can calculate these things in own threads. For audio, it makes
    sense to reuse threads (e.g. render on four threads for four processors,
	no matter if 100 modules are running). For video and byte decompression,
	it may be more confortable to have a blocking implementation in an own
	thread, which is synchronized against the rest of MCOP by the flow system.
<li>Modules may not use MCOP functionality (such as remote invocations) during
    threaded operation
</ul>
<p>
<h4>synchronization</h4>
<p>
Video and midi (and audio) may require synchonization. Basically, that is
timestamping. The idea I have is to attach timestamps to asynchronous streams,
by adding one timestamp to each packet. If you send two video frames, simply
make it two packets (they are large anyway), so that you can have two different
time stamps.
<p>
Audio should implicitely have time stamps, as it is synchronous.
<p>
<h4>dynamic composition</h4>
<p>
It should be possible to say: An effect FX is composed out of these simpler
modules. FX should look like a normal MCOP module (see masquerading), but
in fact consist of other modules.
<p>
This is required for aRtsbuilder.
<p>
<h4>gui</h4>
<p>
All GUI components will be MCOP modules. They should have attributes like
size, label, color, ... . A RAD builder (aRtsbuilder) should be able to
compose them visually.
<p>
The GUI should be saveable by saving the attributes.
<p>
<h4>midi</h4>
<p>
The MIDI stuff will be implemented as asynchronous streams. There are two
options, one is using normal MCOP structures to define the types and the
other is to introduce yet another custom types.
<p>
I think normal structures may be enough, that is something like
<pre>
struct MidiEvent {
  byte b1,b2,b3;
  sequence<byte> sysex;
}
</pre>
<p>
Asynchronous streams should support custom stream types
<p>
<hr>
<a href=index.html>back to index</a>
</body>
</html>
